{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\nogol\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (3.5)\n",
      "Requirement already satisfied: joblib in c:\\users\\nogol\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from nltk) (0.14.1)\n",
      "Requirement already satisfied: click in c:\\users\\nogol\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from nltk) (7.1.2)\n",
      "Requirement already satisfied: tqdm in c:\\users\\nogol\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from nltk) (4.46.0)\n",
      "Requirement already satisfied: regex in c:\\users\\nogol\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from nltk) (2020.5.14)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.5'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'we will learn how to get started with the Natural Language Toolkit Package'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sen=open('sentence.txt')\n",
    "my_sent =sen.read()\n",
    "my_sent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package treebank to\n",
      "[nltk_data]     C:\\Users\\nogol\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package treebank is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('treebank')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Pierre', 'Vinken', ',', '61', 'years', 'old', ',', 'will', 'join', 'the', 'board', 'as', 'a', 'nonexecutive', 'director', 'Nov.', '29', '.'], ['Mr.', 'Vinken', 'is', 'chairman', 'of', 'Elsevier', 'N.V.', ',', 'the', 'Dutch', 'publishing', 'group', '.'], ...]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import treebank\n",
    "treebank.sents()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\nogol\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['we',\n",
       " 'will',\n",
       " 'learn',\n",
       " 'how',\n",
       " 'to',\n",
       " 'get',\n",
       " 'started',\n",
       " 'with',\n",
       " 'the',\n",
       " 'Natural',\n",
       " 'Language',\n",
       " 'Toolkit',\n",
       " 'Package']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "tokens = word_tokenize(my_sent)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Sun rises in the east.', 'Sun sets in the west.']\n"
     ]
    }
   ],
   "source": [
    "sentence_data = \"Sun rises in the east. Sun sets in the west.\"\n",
    "nltk_tokens = nltk.sent_tokenize(sentence_data)\n",
    "print (nltk_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['natural language processing is good.', 'The systems developed']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "f=open('corpus.txt')\n",
    "corpus=f.read()\n",
    "PunktSentenceTokenizer(corpus).tokenize('natural language processing is good. The systems developed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'go'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SnowballStemmer('english').stem(\"going\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual: It  Stem: It\n",
      "Actual: originated  Stem: origin\n",
      "Actual: from  Stem: from\n",
      "Actual: the  Stem: the\n",
      "Actual: idea  Stem: idea\n",
      "Actual: that  Stem: that\n",
      "Actual: there  Stem: there\n",
      "Actual: are  Stem: are\n",
      "Actual: readers  Stem: reader\n",
      "Actual: who  Stem: who\n",
      "Actual: prefer  Stem: prefer\n",
      "Actual: learning  Stem: learn\n",
      "Actual: new  Stem: new\n",
      "Actual: skills  Stem: skill\n",
      "Actual: from  Stem: from\n",
      "Actual: the  Stem: the\n",
      "Actual: comforts  Stem: comfort\n",
      "Actual: of  Stem: of\n",
      "Actual: their  Stem: their\n",
      "Actual: drawing  Stem: draw\n",
      "Actual: rooms  Stem: room\n"
     ]
    }
   ],
   "source": [
    "word_data = \"It originated from the idea that there are readers who prefer learning new skills from the comforts of their drawing rooms\"\n",
    "nltk_tokens = nltk.word_tokenize(word_data)\n",
    "for w in nltk_tokens:\n",
    "       print(\"Actual: %s  Stem: %s\"  % (w,PorterStemmer().stem(w)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\nogol\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual: It  Lemma: It\n",
      "Actual: originated  Lemma: originated\n",
      "Actual: from  Lemma: from\n",
      "Actual: the  Lemma: the\n",
      "Actual: idea  Lemma: idea\n",
      "Actual: that  Lemma: that\n",
      "Actual: there  Lemma: there\n",
      "Actual: are  Lemma: are\n",
      "Actual: readers  Lemma: reader\n",
      "Actual: who  Lemma: who\n",
      "Actual: prefer  Lemma: prefer\n",
      "Actual: learning  Lemma: learning\n",
      "Actual: new  Lemma: new\n",
      "Actual: skills  Lemma: skill\n",
      "Actual: from  Lemma: from\n",
      "Actual: the  Lemma: the\n",
      "Actual: comforts  Lemma: comfort\n",
      "Actual: of  Lemma: of\n",
      "Actual: their  Lemma: their\n",
      "Actual: drawing  Lemma: drawing\n",
      "Actual: rooms  Lemma: room\n"
     ]
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "word_data = \"It originated from the idea that there are readers who prefer learning new skills from the comforts of their drawing rooms\"\n",
    "nltk_tokens = nltk.word_tokenize(word_data)\n",
    "for w in nltk_tokens:\n",
    "       print(\"Actual: %s  Lemma: %s\"  % (w,WordNetLemmatizer().lemmatize(w)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'best'), ('best', 'performance'), ('performance', 'can'), ('can', 'bring'), ('bring', 'in'), ('in', 'sky'), ('sky', 'high'), ('high', 'success'), ('success', '.')]\n"
     ]
    }
   ],
   "source": [
    "word_data = \"The best performance can bring in sky high success.\"\n",
    "nltk_tokens = nltk.word_tokenize(word_data) \n",
    "\n",
    "print(list(nltk.bigrams(nltk_tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('Pierre', 'NNP'), None),\n",
       " (('Vinken', 'NNP'), None),\n",
       " ((',', ','), None),\n",
       " (('61', 'CD'), None),\n",
       " (('years', 'NNS'), None),\n",
       " (('old', 'JJ'), None),\n",
       " ((',', ','), None),\n",
       " (('will', 'MD'), None),\n",
       " (('join', 'VB'), None),\n",
       " (('the', 'DT'), None),\n",
       " (('board', 'NN'), None),\n",
       " (('as', 'IN'), None),\n",
       " (('a', 'DT'), None),\n",
       " (('nonexecutive', 'JJ'), None),\n",
       " (('director', 'NN'), None),\n",
       " (('Nov.', 'NNP'), None),\n",
       " (('29', 'CD'), None),\n",
       " (('.', '.'), None)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tag import UnigramTagger\n",
    "tagged_sents=treebank.tagged_sents()\n",
    "train_sents=tagged_sents[:3000]\n",
    "tagger=UnigramTagger(train_sents)\n",
    "tagger.tag(train_sents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8571551910209367"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sents=tagged_sents[3000:]\n",
    "tagger.evaluate(test_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8571551910209367"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagger=UnigramTagger(train_sents)\n",
    "tagger.evaluate(test_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11318799913662854"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tag import BigramTagger,TrigramTagger\n",
    "bitagger=BigramTagger(train_sents)\n",
    "bitagger.evaluate(test_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.06902654867256637"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tritagger=TrigramTagger(train_sents)\n",
    "tritagger.evaluate(test_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.058493416792575005"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tag import NgramTagger\n",
    "quadtagger=NgramTagger(4,train_sents)\n",
    "quadtagger.evaluate(test_sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemma for studies is study\n",
      "Lemma for studying is studying\n",
      "Lemma for cries is cry\n",
      "Lemma for cry is cry\n"
     ]
    }
   ],
   "source": [
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "text = \"studies studying cries cry\"\n",
    "tokenization = nltk.word_tokenize(text)\n",
    "for w in tokenization:\n",
    "    print(\"Lemma for {} is {}\".format(w, wordnet_lemmatizer.lemmatize(w)))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'belief'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer=WordNetLemmatizer()\n",
    "lemmatizer.lemmatize('believes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### synonyms/antonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('love.n.01'),\n",
       " Synset('love.n.02'),\n",
       " Synset('beloved.n.01'),\n",
       " Synset('love.n.04'),\n",
       " Synset('love.n.05'),\n",
       " Synset('sexual_love.n.02'),\n",
       " Synset('love.v.01'),\n",
       " Synset('love.v.02'),\n",
       " Synset('love.v.03'),\n",
       " Synset('sleep_together.v.01')]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "syn=wordnet.synsets('love')\n",
    "syn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word and Type : travel.n.01\n",
      "Synonym of Travel is: travel\n",
      "The meaning of the word : the act of going from one place to another\n",
      "Example of Travel : ['he enjoyed selling but he hated the travel']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet   #Import wordnet from the NLTK\n",
    "synset = wordnet.synsets(\"Travel\")\n",
    "print('Word and Type : ' + synset[0].name())\n",
    "print('Synonym of Travel is: ' + synset[0].lemmas()[0].name())\n",
    "print('The meaning of the word : ' + synset[0].definition())\n",
    "print('Example of Travel : ' + str(synset[0].examples()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synonyms: ['worse', 'worse', 'worse', 'worsened', 'bad', 'bad', 'big', 'bad', 'tough', 'bad', 'spoiled', 'spoilt', 'regretful', 'sorry', 'bad', 'bad', 'uncollectible', 'bad', 'bad', 'bad', 'risky', 'high-risk', 'speculative', 'bad', 'unfit', 'unsound', 'bad', 'bad', 'bad', 'forged', 'bad', 'defective', 'worse']\n",
      "Antonyms: ['better', 'better', 'good', 'unregretful']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet   #Import wordnet from the NLTK\n",
    "syn = list()\n",
    "ant = list()\n",
    "for synset in wordnet.synsets(\"Worse\"):\n",
    "    for lemma in synset.lemmas():\n",
    "       syn.append(lemma.name())    #add the synonyms\n",
    "       if lemma.antonyms():    #When antonyms are available, add them into the list\n",
    "        ant.append(lemma.antonyms()[0].name())\n",
    "print('Synonyms: ' + str(syn))\n",
    "print('Antonyms: ' + str(ant))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pos tager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\nogol\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('we', 'PRP'),\n",
       " ('will', 'MD'),\n",
       " ('learn', 'VB'),\n",
       " ('how', 'WRB'),\n",
       " ('to', 'TO'),\n",
       " ('get', 'VB'),\n",
       " ('started', 'VBN'),\n",
       " ('with', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('Natural', 'NNP'),\n",
       " ('Language', 'NNP'),\n",
       " ('Toolkit', 'NNP'),\n",
       " ('Package', 'NNP')]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sen=open('sentence.txt')\n",
    "my_sent =sen.read()\n",
    "\n",
    "tokens =word_tokenize(my_sent)\n",
    "\n",
    "tagged = nltk.pos_tag(tokens)\n",
    "tagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('Pierre', 'NNP'), ('Vinken', 'NNP'), (',', ','), ('61', 'CD'), ('years', 'NNS'), ('old', 'JJ'), (',', ','), ('will', 'MD'), ('join', 'VB'), ('the', 'DT'), ('board', 'NN'), ('as', 'IN'), ('a', 'DT'), ('nonexecutive', 'JJ'), ('director', 'NN'), ('Nov.', 'NNP'), ('29', 'CD'), ('.', '.')], [('Mr.', 'NNP'), ('Vinken', 'NNP'), ('is', 'VBZ'), ('chairman', 'NN'), ('of', 'IN'), ('Elsevier', 'NNP'), ('N.V.', 'NNP'), (',', ','), ('the', 'DT'), ('Dutch', 'NNP'), ('publishing', 'VBG'), ('group', 'NN'), ('.', '.')], ...]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagged_sents=treebank.tagged_sents()\n",
    "\n",
    "a=tagged_sents[1:1000] #train\n",
    "b=tagged_sents[500:1000] #test\n",
    "tagged_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9811815150071462"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tag.sequential import ClassifierBasedPOSTagger\n",
    "tagger=ClassifierBasedPOSTagger(train=a)\n",
    "tagger.evaluate(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('This', 'NN'), ('is', 'NN'), ('defualt', 'NN'), ('tagger', 'NN')]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tag import DefaultTagger,\n",
    "tagger=DefaultTagger(\"NN\")\n",
    "tagger.tag([\"This\",\"is\",\"defualt\",\"tagger\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.27973638240431953"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tag import AffixTagger\n",
    "\n",
    "tagger=AffixTagger(a)\n",
    "tagger.evaluate(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.875545003237643"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tag import tnt\n",
    "tnt_tagger=tnt.TnT()\n",
    "tnt_tagger.train(train_sents)\n",
    "tnt_tagger.evaluate(test_sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### normalization and clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the',\n",
       " 'history',\n",
       " 'of',\n",
       " 'natural',\n",
       " 'language',\n",
       " 'processing',\n",
       " '(',\n",
       " 'nlp',\n",
       " ')',\n",
       " 'generally',\n",
       " 'started',\n",
       " 'in',\n",
       " 'the',\n",
       " '1950s',\n",
       " ',',\n",
       " 'although',\n",
       " 'work',\n",
       " 'can',\n",
       " 'be',\n",
       " 'found',\n",
       " 'from',\n",
       " 'earlier',\n",
       " 'periods',\n",
       " '.',\n",
       " 'in',\n",
       " '1950',\n",
       " ',',\n",
       " 'alan',\n",
       " 'turing',\n",
       " 'published',\n",
       " 'an',\n",
       " 'article',\n",
       " 'titled',\n",
       " '``',\n",
       " 'computing',\n",
       " 'machinery',\n",
       " 'and',\n",
       " 'intelligence',\n",
       " \"''\",\n",
       " 'which',\n",
       " 'proposed',\n",
       " 'what',\n",
       " 'is',\n",
       " 'now',\n",
       " 'called',\n",
       " 'the',\n",
       " 'turing',\n",
       " 'test',\n",
       " 'as',\n",
       " 'a',\n",
       " 'criterion',\n",
       " 'of',\n",
       " 'intelligence',\n",
       " '[',\n",
       " 'clarification',\n",
       " 'needed',\n",
       " ']',\n",
       " '.',\n",
       " 'the',\n",
       " 'georgetown',\n",
       " 'experiment',\n",
       " 'in',\n",
       " '1954',\n",
       " 'involved',\n",
       " 'fully',\n",
       " 'automatic',\n",
       " 'translation',\n",
       " 'of',\n",
       " 'more',\n",
       " 'than',\n",
       " 'sixty',\n",
       " 'russian',\n",
       " 'sentences',\n",
       " 'into',\n",
       " 'english',\n",
       " '.',\n",
       " 'the',\n",
       " 'authors',\n",
       " 'claimed',\n",
       " 'that',\n",
       " 'within',\n",
       " 'three',\n",
       " 'or',\n",
       " 'five',\n",
       " 'years',\n",
       " ',',\n",
       " 'machine',\n",
       " 'translation',\n",
       " 'would',\n",
       " 'be',\n",
       " 'a',\n",
       " 'solved',\n",
       " 'problem',\n",
       " '.',\n",
       " '[',\n",
       " '2',\n",
       " ']',\n",
       " 'however',\n",
       " ',',\n",
       " 'real',\n",
       " 'progress',\n",
       " 'was',\n",
       " 'much',\n",
       " 'slower',\n",
       " ',',\n",
       " 'and',\n",
       " 'after',\n",
       " 'the',\n",
       " 'alpac',\n",
       " 'report',\n",
       " 'in',\n",
       " '1966',\n",
       " ',',\n",
       " 'which',\n",
       " 'found',\n",
       " 'that',\n",
       " 'ten-year-long',\n",
       " 'research',\n",
       " 'had',\n",
       " 'failed',\n",
       " 'to',\n",
       " 'fulfill',\n",
       " 'the',\n",
       " 'expectations',\n",
       " ',',\n",
       " 'funding',\n",
       " 'for',\n",
       " 'machine',\n",
       " 'translation',\n",
       " 'was',\n",
       " 'dramatically',\n",
       " 'reduced',\n",
       " '.',\n",
       " 'little',\n",
       " 'further',\n",
       " 'research',\n",
       " 'in',\n",
       " 'machine',\n",
       " 'translation',\n",
       " 'was',\n",
       " 'conducted',\n",
       " 'until',\n",
       " 'the',\n",
       " 'late',\n",
       " '1980s',\n",
       " 'when',\n",
       " 'the',\n",
       " 'first',\n",
       " 'statistical',\n",
       " 'machine',\n",
       " 'translation',\n",
       " 'systems',\n",
       " 'were',\n",
       " 'developed',\n",
       " '.',\n",
       " 'some',\n",
       " 'notably',\n",
       " 'successful',\n",
       " 'natural',\n",
       " 'language',\n",
       " 'processing',\n",
       " 'systems',\n",
       " 'developed',\n",
       " 'in',\n",
       " 'the',\n",
       " '1960s',\n",
       " 'were',\n",
       " 'shrdlu',\n",
       " ',',\n",
       " 'a',\n",
       " 'natural',\n",
       " 'language',\n",
       " 'system',\n",
       " 'working',\n",
       " 'in',\n",
       " 'restricted',\n",
       " '``',\n",
       " 'blocks',\n",
       " 'worlds',\n",
       " \"''\",\n",
       " 'with',\n",
       " 'restricted',\n",
       " 'vocabularies',\n",
       " ',',\n",
       " 'and',\n",
       " 'eliza',\n",
       " ',',\n",
       " 'a',\n",
       " 'simulation',\n",
       " 'of',\n",
       " 'a',\n",
       " 'rogerian',\n",
       " 'psychotherapist',\n",
       " ',',\n",
       " 'written',\n",
       " 'by',\n",
       " 'joseph',\n",
       " 'weizenbaum',\n",
       " 'between',\n",
       " '1964',\n",
       " 'and',\n",
       " '1966.',\n",
       " 'using',\n",
       " 'almost',\n",
       " 'no',\n",
       " 'information',\n",
       " 'about',\n",
       " 'human',\n",
       " 'thought',\n",
       " 'or',\n",
       " 'emotion',\n",
       " ',',\n",
       " 'eliza',\n",
       " 'sometimes',\n",
       " 'provided',\n",
       " 'a',\n",
       " 'startlingly',\n",
       " 'human-like',\n",
       " 'interaction',\n",
       " '.',\n",
       " 'when',\n",
       " 'the',\n",
       " '``',\n",
       " 'patient',\n",
       " \"''\",\n",
       " 'exceeded',\n",
       " 'the',\n",
       " 'very',\n",
       " 'small',\n",
       " 'knowledge',\n",
       " 'base',\n",
       " ',',\n",
       " 'eliza',\n",
       " 'might',\n",
       " 'provide',\n",
       " 'a',\n",
       " 'generic',\n",
       " 'response',\n",
       " ',',\n",
       " 'for',\n",
       " 'example',\n",
       " ',',\n",
       " 'responding',\n",
       " 'to',\n",
       " '``',\n",
       " 'my',\n",
       " 'head',\n",
       " 'hurts',\n",
       " \"''\",\n",
       " 'with',\n",
       " '``',\n",
       " 'why',\n",
       " 'do',\n",
       " 'you',\n",
       " 'say',\n",
       " 'your',\n",
       " 'head',\n",
       " 'hurts',\n",
       " '?',\n",
       " \"''\",\n",
       " '.',\n",
       " 'during',\n",
       " 'the',\n",
       " '1970s',\n",
       " ',',\n",
       " 'many',\n",
       " 'programmers',\n",
       " 'began',\n",
       " 'to',\n",
       " 'write',\n",
       " '``',\n",
       " 'conceptual',\n",
       " 'ontologies',\n",
       " \"''\",\n",
       " ',',\n",
       " 'which',\n",
       " 'structured',\n",
       " 'real-world',\n",
       " 'information',\n",
       " 'into',\n",
       " 'computer-understandable',\n",
       " 'data',\n",
       " '.',\n",
       " 'examples',\n",
       " 'are',\n",
       " 'margie',\n",
       " '(',\n",
       " 'schank',\n",
       " ',',\n",
       " '1975',\n",
       " ')',\n",
       " ',',\n",
       " 'sam',\n",
       " '(',\n",
       " 'cullingford',\n",
       " ',',\n",
       " '1978',\n",
       " ')',\n",
       " ',',\n",
       " 'pam',\n",
       " '(',\n",
       " 'wilensky',\n",
       " ',',\n",
       " '1978',\n",
       " ')',\n",
       " ',',\n",
       " 'talespin',\n",
       " '(',\n",
       " 'meehan',\n",
       " ',',\n",
       " '1976',\n",
       " ')',\n",
       " ',',\n",
       " 'qualm',\n",
       " '(',\n",
       " 'lehnert',\n",
       " ',',\n",
       " '1977',\n",
       " ')',\n",
       " ',',\n",
       " 'politics',\n",
       " '(',\n",
       " 'carbonell',\n",
       " ',',\n",
       " '1979',\n",
       " ')',\n",
       " ',',\n",
       " 'and',\n",
       " 'plot',\n",
       " 'units',\n",
       " '(',\n",
       " 'lehnert',\n",
       " '1981',\n",
       " ')',\n",
       " '.',\n",
       " 'during',\n",
       " 'this',\n",
       " 'time',\n",
       " ',',\n",
       " 'many',\n",
       " 'chatterbots',\n",
       " 'were',\n",
       " 'written',\n",
       " 'including',\n",
       " 'parry',\n",
       " ',',\n",
       " 'racter',\n",
       " ',',\n",
       " 'and',\n",
       " 'jabberwacky',\n",
       " '.',\n",
       " 'up',\n",
       " 'to',\n",
       " 'the',\n",
       " '1980s',\n",
       " ',',\n",
       " 'most',\n",
       " 'natural',\n",
       " 'language',\n",
       " 'processing',\n",
       " 'systems',\n",
       " 'were',\n",
       " 'based',\n",
       " 'on',\n",
       " 'complex',\n",
       " 'sets',\n",
       " 'of',\n",
       " 'hand-written',\n",
       " 'rules',\n",
       " '.',\n",
       " 'starting',\n",
       " 'in',\n",
       " 'the',\n",
       " 'late',\n",
       " '1980s',\n",
       " ',',\n",
       " 'however',\n",
       " ',',\n",
       " 'there',\n",
       " 'was',\n",
       " 'a',\n",
       " 'revolution',\n",
       " 'in',\n",
       " 'natural',\n",
       " 'language',\n",
       " 'processing',\n",
       " 'with',\n",
       " 'the',\n",
       " 'introduction',\n",
       " 'of',\n",
       " 'machine',\n",
       " 'learning',\n",
       " 'algorithms',\n",
       " 'for',\n",
       " 'language',\n",
       " 'processing',\n",
       " '.',\n",
       " 'this',\n",
       " 'was',\n",
       " 'due',\n",
       " 'to',\n",
       " 'both',\n",
       " 'the',\n",
       " 'steady',\n",
       " 'increase',\n",
       " 'in',\n",
       " 'computational',\n",
       " 'power',\n",
       " '(',\n",
       " 'see',\n",
       " 'moore',\n",
       " \"'s\",\n",
       " 'law',\n",
       " ')',\n",
       " 'and',\n",
       " 'the',\n",
       " 'gradual',\n",
       " 'lessening',\n",
       " 'of',\n",
       " 'the',\n",
       " 'dominance',\n",
       " 'of',\n",
       " 'chomskyan',\n",
       " 'theories',\n",
       " 'of',\n",
       " 'linguistics',\n",
       " '(',\n",
       " 'e.g',\n",
       " '.',\n",
       " 'transformational',\n",
       " 'grammar',\n",
       " ')',\n",
       " ',',\n",
       " 'whose',\n",
       " 'theoretical',\n",
       " 'underpinnings',\n",
       " 'discouraged',\n",
       " 'the',\n",
       " 'sort',\n",
       " 'of',\n",
       " 'corpus',\n",
       " 'linguistics',\n",
       " 'that',\n",
       " 'underlies',\n",
       " 'the',\n",
       " 'machine-learning',\n",
       " 'approach',\n",
       " 'to',\n",
       " 'language',\n",
       " 'processing',\n",
       " '.',\n",
       " '[',\n",
       " '3',\n",
       " ']',\n",
       " 'some',\n",
       " 'of',\n",
       " 'the',\n",
       " 'earliest-used',\n",
       " 'machine',\n",
       " 'learning',\n",
       " 'algorithms',\n",
       " ',',\n",
       " 'such',\n",
       " 'as',\n",
       " 'decision',\n",
       " 'trees',\n",
       " ',',\n",
       " 'produced',\n",
       " 'systems',\n",
       " 'of',\n",
       " 'hard',\n",
       " 'if-then',\n",
       " 'rules',\n",
       " 'similar',\n",
       " 'to',\n",
       " 'existing',\n",
       " 'hand-written',\n",
       " 'rules',\n",
       " '.',\n",
       " 'however',\n",
       " ',',\n",
       " 'part-of-speech',\n",
       " 'tagging',\n",
       " 'introduced',\n",
       " 'the',\n",
       " 'use',\n",
       " 'of',\n",
       " 'hidden',\n",
       " 'markov',\n",
       " 'models',\n",
       " 'to',\n",
       " 'natural',\n",
       " 'language',\n",
       " 'processing',\n",
       " ',',\n",
       " 'and',\n",
       " 'increasingly',\n",
       " ',',\n",
       " 'research',\n",
       " 'has',\n",
       " 'focused',\n",
       " 'on',\n",
       " 'statistical',\n",
       " 'models',\n",
       " ',',\n",
       " 'which',\n",
       " 'make',\n",
       " 'soft',\n",
       " ',',\n",
       " 'probabilistic',\n",
       " 'decisions',\n",
       " 'based',\n",
       " 'on',\n",
       " 'attaching',\n",
       " 'real-valued',\n",
       " 'weights',\n",
       " 'to',\n",
       " 'the',\n",
       " 'features',\n",
       " 'making',\n",
       " 'up',\n",
       " 'the',\n",
       " 'input',\n",
       " 'data',\n",
       " '.',\n",
       " 'the',\n",
       " 'cache',\n",
       " 'language',\n",
       " 'models',\n",
       " 'upon',\n",
       " 'which',\n",
       " 'many',\n",
       " 'speech',\n",
       " 'recognition',\n",
       " 'systems',\n",
       " 'now',\n",
       " 'rely',\n",
       " 'are',\n",
       " 'examples',\n",
       " 'of',\n",
       " 'such',\n",
       " 'statistical',\n",
       " 'models',\n",
       " '.',\n",
       " 'such',\n",
       " 'models',\n",
       " 'are',\n",
       " 'generally',\n",
       " 'more',\n",
       " 'robust',\n",
       " 'when',\n",
       " 'given',\n",
       " 'unfamiliar',\n",
       " 'input',\n",
       " ',',\n",
       " 'especially',\n",
       " 'input',\n",
       " 'that',\n",
       " 'contains',\n",
       " 'errors',\n",
       " '(',\n",
       " 'as',\n",
       " 'is',\n",
       " 'very',\n",
       " 'common',\n",
       " 'for',\n",
       " 'real-world',\n",
       " 'data',\n",
       " ')',\n",
       " ',',\n",
       " 'and',\n",
       " 'produce',\n",
       " 'more',\n",
       " 'reliable',\n",
       " 'results',\n",
       " 'when',\n",
       " 'integrated',\n",
       " 'into',\n",
       " 'a',\n",
       " 'larger',\n",
       " 'system',\n",
       " 'comprising',\n",
       " 'multiple',\n",
       " 'subtasks',\n",
       " '.',\n",
       " 'many',\n",
       " 'of',\n",
       " 'the',\n",
       " 'notable',\n",
       " 'early',\n",
       " 'successes',\n",
       " 'occurred',\n",
       " 'in',\n",
       " 'the',\n",
       " 'field',\n",
       " 'of',\n",
       " 'machine',\n",
       " 'translation',\n",
       " ',',\n",
       " 'due',\n",
       " 'especially',\n",
       " 'to',\n",
       " 'work',\n",
       " 'at',\n",
       " 'ibm',\n",
       " 'research',\n",
       " ',',\n",
       " 'where',\n",
       " 'successively',\n",
       " 'more',\n",
       " 'complicated',\n",
       " 'statistical',\n",
       " 'models',\n",
       " 'were',\n",
       " 'developed',\n",
       " '.',\n",
       " 'these',\n",
       " 'systems',\n",
       " 'were',\n",
       " 'able',\n",
       " 'to',\n",
       " 'take',\n",
       " 'advantage',\n",
       " 'of',\n",
       " 'existing',\n",
       " 'multilingual',\n",
       " 'textual',\n",
       " 'corpora',\n",
       " 'that',\n",
       " 'had',\n",
       " 'been',\n",
       " 'produced',\n",
       " 'by',\n",
       " 'the',\n",
       " 'parliament',\n",
       " 'of',\n",
       " 'canada',\n",
       " 'and',\n",
       " 'the',\n",
       " 'european',\n",
       " 'union',\n",
       " 'as',\n",
       " 'a',\n",
       " 'result',\n",
       " 'of',\n",
       " 'laws',\n",
       " 'calling',\n",
       " 'for',\n",
       " 'the',\n",
       " 'translation',\n",
       " 'of',\n",
       " 'all',\n",
       " 'governmental',\n",
       " 'proceedings',\n",
       " 'into',\n",
       " 'all',\n",
       " 'official',\n",
       " 'languages',\n",
       " 'of',\n",
       " 'the',\n",
       " 'corresponding',\n",
       " 'systems',\n",
       " 'of',\n",
       " 'government',\n",
       " '.',\n",
       " 'however',\n",
       " ',',\n",
       " 'most',\n",
       " 'other',\n",
       " 'systems',\n",
       " 'depended',\n",
       " 'on',\n",
       " 'corpora',\n",
       " 'specifically',\n",
       " 'developed',\n",
       " 'for',\n",
       " 'the',\n",
       " 'tasks',\n",
       " 'implemented',\n",
       " 'by',\n",
       " 'these',\n",
       " 'systems',\n",
       " ',',\n",
       " 'which',\n",
       " 'was',\n",
       " '(',\n",
       " 'and',\n",
       " 'often',\n",
       " 'continues',\n",
       " 'to',\n",
       " 'be',\n",
       " ')',\n",
       " 'a',\n",
       " 'major',\n",
       " 'limitation',\n",
       " 'in',\n",
       " 'the',\n",
       " 'success',\n",
       " 'of',\n",
       " 'these',\n",
       " 'systems',\n",
       " '.',\n",
       " 'as',\n",
       " 'a',\n",
       " 'result',\n",
       " ',',\n",
       " 'a',\n",
       " 'great',\n",
       " 'deal',\n",
       " 'of',\n",
       " 'research',\n",
       " 'has',\n",
       " 'gone',\n",
       " 'into',\n",
       " 'methods',\n",
       " 'of',\n",
       " 'more',\n",
       " 'effectively',\n",
       " 'learning',\n",
       " 'from',\n",
       " 'limited',\n",
       " 'amounts',\n",
       " 'of',\n",
       " 'data',\n",
       " '.',\n",
       " 'recent',\n",
       " 'research',\n",
       " 'has',\n",
       " 'increasingly',\n",
       " 'focused',\n",
       " 'on',\n",
       " 'unsupervised',\n",
       " 'and',\n",
       " 'semi-supervised',\n",
       " 'learning',\n",
       " 'algorithms',\n",
       " '.',\n",
       " 'such',\n",
       " 'algorithms',\n",
       " 'can',\n",
       " 'learn',\n",
       " 'from',\n",
       " 'data',\n",
       " 'that',\n",
       " 'has',\n",
       " 'not',\n",
       " 'been',\n",
       " 'hand-annotated',\n",
       " 'with',\n",
       " 'the',\n",
       " 'desired',\n",
       " 'answers',\n",
       " 'or',\n",
       " 'using',\n",
       " 'a',\n",
       " 'combination',\n",
       " 'of',\n",
       " 'annotated',\n",
       " 'and',\n",
       " 'non-annotated',\n",
       " 'data',\n",
       " '.',\n",
       " 'generally',\n",
       " ',',\n",
       " 'this',\n",
       " 'task',\n",
       " 'is',\n",
       " 'much',\n",
       " 'more',\n",
       " 'difficult',\n",
       " 'than',\n",
       " 'supervised',\n",
       " 'learning',\n",
       " ',',\n",
       " 'and',\n",
       " 'typically',\n",
       " 'produces',\n",
       " 'less',\n",
       " 'accurate',\n",
       " 'results',\n",
       " 'for',\n",
       " 'a',\n",
       " 'given',\n",
       " 'amount',\n",
       " 'of',\n",
       " 'input',\n",
       " 'data',\n",
       " '.',\n",
       " 'however',\n",
       " ',',\n",
       " 'there',\n",
       " 'is',\n",
       " 'an',\n",
       " 'enormous',\n",
       " 'amount',\n",
       " 'of',\n",
       " 'non-annotated',\n",
       " 'data',\n",
       " 'available',\n",
       " '(',\n",
       " 'including',\n",
       " ',',\n",
       " 'among',\n",
       " 'other',\n",
       " 'things',\n",
       " ',',\n",
       " 'the',\n",
       " 'entire',\n",
       " 'content',\n",
       " 'of',\n",
       " 'the',\n",
       " 'world',\n",
       " 'wide',\n",
       " 'web',\n",
       " ')',\n",
       " ',',\n",
       " 'which',\n",
       " 'can',\n",
       " 'often',\n",
       " 'make',\n",
       " 'up',\n",
       " 'for',\n",
       " 'the',\n",
       " 'inferior',\n",
       " 'results',\n",
       " 'if',\n",
       " 'the',\n",
       " 'algorithm',\n",
       " 'used',\n",
       " 'has',\n",
       " 'a',\n",
       " 'low',\n",
       " 'enough',\n",
       " 'time',\n",
       " 'complexity',\n",
       " 'to',\n",
       " 'be',\n",
       " 'practical',\n",
       " '.',\n",
       " 'in',\n",
       " 'the',\n",
       " '2010s',\n",
       " ',',\n",
       " 'representation',\n",
       " 'learning',\n",
       " 'and',\n",
       " 'deep',\n",
       " 'neural',\n",
       " 'network-style',\n",
       " 'machine',\n",
       " 'learning',\n",
       " 'methods',\n",
       " 'became',\n",
       " 'widespread',\n",
       " 'in',\n",
       " 'natural',\n",
       " 'language',\n",
       " 'processing',\n",
       " ',',\n",
       " 'due',\n",
       " 'in',\n",
       " 'part',\n",
       " 'to',\n",
       " 'a',\n",
       " 'flurry',\n",
       " 'of',\n",
       " 'results',\n",
       " 'showing',\n",
       " 'that',\n",
       " 'such',\n",
       " 'techniques',\n",
       " '[',\n",
       " '4',\n",
       " ']',\n",
       " '[',\n",
       " '5',\n",
       " ']',\n",
       " 'can',\n",
       " 'achieve',\n",
       " 'state-of-the-art',\n",
       " 'results',\n",
       " 'in',\n",
       " 'many',\n",
       " 'natural',\n",
       " 'language',\n",
       " 'tasks',\n",
       " ',',\n",
       " 'for',\n",
       " 'example',\n",
       " 'in',\n",
       " 'language',\n",
       " 'modeling',\n",
       " ',',\n",
       " '[',\n",
       " '6',\n",
       " ']',\n",
       " 'parsing',\n",
       " ',',\n",
       " '[',\n",
       " '7',\n",
       " ']',\n",
       " '[',\n",
       " '8',\n",
       " ']',\n",
       " 'and',\n",
       " 'many',\n",
       " 'others',\n",
       " '.',\n",
       " 'popular',\n",
       " 'techniques',\n",
       " 'include',\n",
       " 'the',\n",
       " 'use',\n",
       " 'of',\n",
       " 'word',\n",
       " 'embeddings',\n",
       " 'to',\n",
       " 'capture',\n",
       " 'semantic',\n",
       " 'properties',\n",
       " 'of',\n",
       " 'words',\n",
       " ',',\n",
       " 'and',\n",
       " 'an',\n",
       " 'increase',\n",
       " 'in',\n",
       " 'end-to-end',\n",
       " 'learning',\n",
       " 'of',\n",
       " 'a',\n",
       " 'higher-level',\n",
       " 'task',\n",
       " '(',\n",
       " 'e.g.',\n",
       " ',',\n",
       " 'question',\n",
       " 'answering',\n",
       " ')',\n",
       " 'instead',\n",
       " 'of',\n",
       " 'relying',\n",
       " 'on',\n",
       " 'a',\n",
       " 'pipeline',\n",
       " 'of',\n",
       " 'separate',\n",
       " 'intermediate',\n",
       " 'tasks',\n",
       " '(',\n",
       " 'e.g.',\n",
       " ',',\n",
       " 'part-of-speech',\n",
       " 'tagging',\n",
       " 'and',\n",
       " 'dependency',\n",
       " 'parsing',\n",
       " ')',\n",
       " '.',\n",
       " 'in',\n",
       " 'some',\n",
       " 'areas',\n",
       " ',',\n",
       " 'this',\n",
       " 'shift',\n",
       " 'has',\n",
       " 'entailed',\n",
       " 'substantial',\n",
       " 'changes',\n",
       " 'in',\n",
       " 'how',\n",
       " 'nlp',\n",
       " 'systems',\n",
       " 'are',\n",
       " 'designed',\n",
       " ',',\n",
       " 'such',\n",
       " 'that',\n",
       " 'deep',\n",
       " 'neural',\n",
       " 'network-based',\n",
       " 'approaches',\n",
       " 'may',\n",
       " 'be',\n",
       " 'viewed',\n",
       " 'as',\n",
       " 'a',\n",
       " 'new',\n",
       " 'paradigm',\n",
       " 'distinct',\n",
       " 'from',\n",
       " 'statistical',\n",
       " 'natural',\n",
       " 'language',\n",
       " 'processing',\n",
       " '.',\n",
       " 'for',\n",
       " 'instance',\n",
       " ',',\n",
       " 'the',\n",
       " ...]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "f=open('corpus.txt')\n",
    "corpus=f.read()\n",
    "corpus=corpus.lower()\n",
    "nltk.word_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1046"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nltk.word_tokenize(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 3), match='the'>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "result = re.match(r\"[a-zA-z]+\", corpus)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\nogol\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\nogol\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('brown')\n",
    "from nltk.corpus import brown\n",
    "brown.categories()\n",
    "len(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### metrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.metrics import edit_distance\n",
    "edit_distance('book','booooooooook')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'NN': 7, 'VB': 1})"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.probability import ConditionalFreqDist\n",
    "from nltk.corpus import treebank\n",
    "ConditionalFreqDist(treebank.tagged_words())['book']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'NN': 5, ',': 2, 'VBZ': 1, 'CD': 1, 'IN': 1, 'DT': 1, 'JJS': 1, 'NNS': 1, 'TO': 1, 'VB': 1, 'JJ': 1, 'CC': 1, 'RB': 1, 'JJR': 1, '.': 1})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import nltk\n",
    "text = \" Guru99 is one of the best sites to learn WEB, SAP, Ethical Hacking and much more online.\"\n",
    "lower_case = text.lower()\n",
    "tokens = nltk.word_tokenize(lower_case)\n",
    "tags = nltk.pos_tag(tokens)\n",
    "counts = Counter( tag for word,  tag in tags)\n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = [(\"a\", \"DT\"),(\"clever\",\"JJ\"),(\"fox\",\"NN\"),(\"was\",\"VBP\"),(\"jumping\",\"VBP\"),(\"over\",\"IN\"),(\"the\",\"DT\"),(\"wall\",\"NN\")]\n",
    "grammar = \"NP:{<DT>?<JJ>*<NN>}\"\n",
    "parser_chunking = nltk.RegexpParser(grammar)\n",
    "parser_chunking.parse(sentence)\n",
    "output = parser_chunking.parse(sentence)\n",
    "output.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'shot', 'an', 'elephant', 'in', 'my', 'pajamas']\n",
      "--------------------------------\n",
      "[('I', 'PRP'), ('shot', 'VBP'), ('an', 'DT'), ('elephant', 'NN'), ('in', 'IN'), ('my', 'PRP$')]\n",
      "--------------------------------\n",
      "(S\n",
      "  (NP I)\n",
      "  (VP\n",
      "    (VP (V shot) (NP (Det an) (N elephant)))\n",
      "    (PP (P in) (NP (Det my) (N pajamas)))))\n",
      "(S\n",
      "  (NP I)\n",
      "  (VP\n",
      "    (V shot)\n",
      "    (NP (Det an) (N elephant) (PP (P in) (NP (Det my) (N pajamas))))))\n"
     ]
    }
   ],
   "source": [
    "grammar = nltk.CFG.fromstring(\"\"\"\n",
    "S -> NP VP\n",
    "PP -> P NP\n",
    "NP -> Det N | Det N PP | 'I'\n",
    "VP -> V NP | VP PP\n",
    "Det -> 'an' | 'my'\n",
    "N -> 'elephant' | 'pajamas'\n",
    "V -> 'shot'\n",
    "P -> 'in'\n",
    "\"\"\")\n",
    "\n",
    "sent = \"\"\"I shot an elephant in my pajamas\"\"\"\n",
    "\n",
    "tokens = nltk.word_tokenize(sent)\n",
    "print(tokens)\n",
    "print(\"--------------------------------\")\n",
    "\n",
    "tagged = nltk.pos_tag(tokens)\n",
    "print(tagged[0:6])\n",
    "print(\"--------------------------------\")\n",
    "\n",
    "parser = nltk.ChartParser(grammar)\n",
    "for t in parser.parse(sent.split()):\n",
    "    print(t)\n",
    "    t.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'angry', 'bear', 'chased', 'the', 'frightened', 'little', 'squirrel']\n",
      "----------------------------------------------------------------\n",
      "[('the', 'DT'), ('angry', 'JJ'), ('bear', 'NN'), ('chased', 'VBD'), ('the', 'DT'), ('frightened', 'JJ')]\n",
      "----------------------------------------------------------------\n",
      "(S\n",
      "  (NP (Det the) (Nom (Adj angry) (Nom (N bear))))\n",
      "  (VP\n",
      "    (V chased)\n",
      "    (NP\n",
      "      (Det the)\n",
      "      (Nom (Adj frightened) (Nom (Adj little) (Nom (N squirrel)))))))\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "grammar= nltk.CFG.fromstring(\"\"\"\n",
    "  S  -> NP VP\n",
    "  NP -> Det Nom | PropN\n",
    "  Nom -> Adj Nom | N\n",
    "  VP -> V Adj | V NP | V S | V NP PP\n",
    "  PP -> P NP\n",
    "  PropN -> 'Buster' | 'Chatterer' | 'Joe'\n",
    "  Det -> 'the' | 'a'\n",
    "  N -> 'bear' | 'squirrel' | 'tree' | 'fish' | 'log'\n",
    "  Adj  -> 'angry' | 'frightened' |  'little' | 'tall'\n",
    "  V ->  'chased'  | 'saw' | 'said' | 'thought' | 'was' | 'put'\n",
    "  P -> 'on'\n",
    "  \"\"\")\n",
    "\n",
    "sent = ['the', 'angry', 'bear', 'chased', 'the', 'frightened', 'little','squirrel']\n",
    "\n",
    "\n",
    "sent = \"\"\"the angry bear chased the frightened little squirrel\"\"\"\n",
    "\n",
    "tokens = nltk.word_tokenize(sent)\n",
    "print(tokens)\n",
    "print(\"----------------------------------------------------------------\")\n",
    "\n",
    "tagged = nltk.pos_tag(tokens)\n",
    "print(tagged[0:6])\n",
    "print(\"----------------------------------------------------------------\")\n",
    "\n",
    "parser = nltk.ShiftReduceParser(grammar)\n",
    "\n",
    "for tree in parser.parse(sent.split()):\n",
    "    print(tree)\n",
    "    tree.draw()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WFST 1    2    3    4    5    6    7   \n",
      "0    NP   .    .    S    .    .    S    \n",
      "1    .    V    .    VP   .    .    VP   \n",
      "2    .    .    Det  NP   .    .    .    \n",
      "3    .    .    .    N    .    .    .    \n",
      "4    .    .    .    .    P    .    PP   \n",
      "5    .    .    .    .    .    Det  NP   \n",
      "6    .    .    .    .    .    .    N    \n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "grammar = nltk.CFG.fromstring(\"\"\"\n",
    "S -> NP VP\n",
    "PP -> P NP\n",
    "NP -> Det N | Det N PP | 'I'\n",
    "VP -> V NP | VP PP\n",
    "Det -> 'an' | 'my'\n",
    "N -> 'elephant' | 'pajamas'\n",
    "V -> 'shot'\n",
    "P -> 'in'\n",
    "\"\"\")\n",
    "\n",
    "tokens = \"I shot an elephant in my pajamas\".split()\n",
    "\n",
    "\n",
    "def init_wfst(tokens, grammar):\n",
    "    numtokens = len(tokens)\n",
    "    wfst = [[None for i in range(numtokens + 1)] for j in range(numtokens + 1)]\n",
    "    for i in range(numtokens):\n",
    "        productions = grammar.productions(rhs=tokens[i])\n",
    "        wfst[i][i + 1] = productions[0].lhs()\n",
    "    return wfst\n",
    "\n",
    "\n",
    "def complete_wfst(wfst, tokens, grammar, trace=False):\n",
    "    index = dict((p.rhs(), p.lhs()) for p in grammar.productions())\n",
    "    numtokens = len(tokens)\n",
    "    for span in range(2, numtokens + 1):\n",
    "        for start in range(numtokens + 1 - span):\n",
    "            end = start + span\n",
    "            for mid in range(start + 1, end):\n",
    "                nt1, nt2 = wfst[start][mid], wfst[mid][end]\n",
    "                if nt1 and nt2 and (nt1, nt2) in index:\n",
    "                    wfst[start][end] = index[(nt1, nt2)]\n",
    "                    if trace:\n",
    "                        print(\"[%s] %3s [%s] %3s [%s] ==> [%s] %3s [%s]\" % \\\n",
    "                              (start, nt1, mid, nt2, end, start, index[(nt1, nt2)], end))\n",
    "    return wfst\n",
    "\n",
    "\n",
    "def display(wfst, tokens):\n",
    "    print('\\nWFST ' + ' '.join((\"%-4d\" % i) for i in range(1, len(wfst))))\n",
    "    for i in range(len(wfst) - 1):\n",
    "        print(\"%d   \" % i, end=\" \")\n",
    "        for j in range(1, len(wfst)):\n",
    "            print(\"%-4s\" % (wfst[i][j] or '.'), end=\" \")\n",
    "        print()\n",
    "\n",
    "\n",
    "\n",
    "wfst0 = init_wfst(tokens, grammar)\n",
    "\n",
    "wfst1 = complete_wfst(wfst0, tokens, grammar)\n",
    "display(wfst1, tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'shot', 'an', 'elephant', 'in', 'my', 'pajamas']\n",
      "----------------------------------------------------------------\n",
      "[('I', 'PRP'), ('shot', 'VBP'), ('an', 'DT'), ('elephant', 'NN'), ('in', 'IN'), ('my', 'PRP$')]\n",
      "----------------------------------------------------------------\n",
      "(shot I (elephant an (in (pajamas my))))\n",
      "(shot I (elephant an) (in (pajamas my)))\n"
     ]
    }
   ],
   "source": [
    "grammar = nltk.DependencyGrammar.fromstring(\"\"\"\n",
    "'shot' -> 'I' | 'elephant' | 'in'\n",
    "'elephant' -> 'an' | 'in'\n",
    "'in' -> 'pajamas'\n",
    "'pajamas' -> 'my'\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "\n",
    "sent = \"\"\"I shot an elephant in my pajamas \"\"\"\n",
    "\n",
    "tokens = nltk.word_tokenize(sent)\n",
    "print(tokens)\n",
    "print(\"----------------------------------------------------------------\")\n",
    "\n",
    "tagged = nltk.pos_tag(tokens)\n",
    "print(tagged[0:6])\n",
    "print(\"----------------------------------------------------------------\")\n",
    "\n",
    "pdp = nltk.ProjectiveDependencyParser(grammar)\n",
    "trees = pdp.parse(sent.split())\n",
    "for tree in trees:\n",
    "    print(tree)\n",
    "    tree.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### text classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\nogol\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from sklearn.datasets import load_files\n",
    "nltk.download('stopwords')\n",
    "import pickle\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_data = load_files(r\"txt_sentoken\")\n",
    "X, y = movie_data.data, movie_data.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = []\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "stemmer = WordNetLemmatizer()\n",
    "\n",
    "for sen in range(0, len(X)):\n",
    "    # Remove all the special characters\n",
    "    document = re.sub(r'\\W', ' ', str(X[sen]))\n",
    "    \n",
    "    # remove all single characters\n",
    "    document = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', document)\n",
    "    \n",
    "    # Remove single characters from the start\n",
    "    document = re.sub(r'\\^[a-zA-Z]\\s+', ' ', document) \n",
    "    \n",
    "    # Substituting multiple spaces with single space\n",
    "    document = re.sub(r'\\s+', ' ', document, flags=re.I)\n",
    "    \n",
    "    # Removing prefixed 'b'\n",
    "    document = re.sub(r'^b\\s+', '', document)\n",
    "    \n",
    "    # Converting to Lowercase\n",
    "    document = document.lower()\n",
    "    \n",
    "    # Lemmatization\n",
    "    document = document.split()\n",
    "\n",
    "    document = [stemmer.lemmatize(word) for word in document]\n",
    "    document = ' '.join(document)\n",
    "    \n",
    "    documents.append(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(max_features=1500, min_df=5, max_df=0.7, stop_words=stopwords.words('english'))\n",
    "X = vectorizer.fit_transform(documents).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidfconverter = TfidfTransformer()\n",
    "X = tfidfconverter.fit_transform(X).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidfconverter = TfidfVectorizer(max_features=1500, min_df=5, max_df=0.7, stop_words=stopwords.words('english'))\n",
    "X = tfidfconverter.fit_transform(documents).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                       criterion='gini', max_depth=None, max_features='auto',\n",
       "                       max_leaf_nodes=None, max_samples=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=1000,\n",
       "                       n_jobs=None, oob_score=False, random_state=0, verbose=0,\n",
       "                       warm_start=False)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "classifier = RandomForestClassifier(n_estimators=1000, random_state=0)\n",
    "classifier.fit(X_train, y_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1,\n",
       "       1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0,\n",
       "       1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0,\n",
       "       0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1,\n",
       "       0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0,\n",
       "       1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1,\n",
       "       0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1,\n",
       "       1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0,\n",
       "       0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0,\n",
       "       0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1,\n",
       "       0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0,\n",
       "       0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0,\n",
       "       0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1,\n",
       "       0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1,\n",
       "       1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0,\n",
       "       1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1,\n",
       "       1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1,\n",
       "       0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1,\n",
       "       0, 0, 1, 0])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = classifier.predict(X_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[180  28]\n",
      " [ 30 162]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.87      0.86       208\n",
      "           1       0.85      0.84      0.85       192\n",
      "\n",
      "    accuracy                           0.85       400\n",
      "   macro avg       0.85      0.85      0.85       400\n",
      "weighted avg       0.85      0.85      0.85       400\n",
      "\n",
      "0.855\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
